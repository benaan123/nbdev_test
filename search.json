[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nbdev_test",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "nbdev_test",
    "section": "Install",
    "text": "Install\npip install nbdev_test"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "nbdev_test",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "Viking_descriptive_stats_how_SG_and_UC_align_baa.html",
    "href": "Viking_descriptive_stats_how_SG_and_UC_align_baa.html",
    "title": "nbdev_test",
    "section": "",
    "text": "import warnings\nimport plotly.express as px\nimport re\nfrom cartoframes.viz import Map, Layer, color_continuous_style\nwarnings.simplefilter(\"once\")\n\n#The aim of this notebook is to compare SafeGraph and Unacast patterns to show where those versions align and where they differ. This should be used to track process for key industries as well.\nReproducibility given by consistent analysis: - restrict to target industries: (‘722511’,‘722513’,‘444130’,‘445110’,‘445120’,‘447110’,‘452319’,‘517312’,‘713990’,‘721110’,‘721120’,‘722515’) - restrict to 2021-01-01 to 2021-03-31 - region: Chicago + Dallas - for trimmed mean remove top and low 0.01 quantile - percentage of places above .55 correlation used\n#Todo:\n\nspider plot:\n\nrun grand summary per naics code\nadjust radar plot to use that output\nadd more metrixs (percent of placekey with sign. drop)\n\nhow many NaN and 0 (what are we dropping)\n\n\n#@title Input tables and filters\nSG_table = \"uc-prox-core-dev.viking_input.combined_weekly_patterns\" #@param {type: \"string\"}\nUC_table = \"uc-prox-core-dev.viking_align_v1_patterns.patterns_final_view_weekly\" #@param {type: \"string\"}\nPLACES_table = \"uc-prox-core-dev.viking_input.combined_places\" #@param{type: \"string\"}\n\n#@markdown restrict to specific industries or in time:\nindustries = \"('722511','722513','444130','445110','445120','447110','452319','517312','713990','721110','721120','722515')\" #@param {type: \"string\"}\nstart_date = \"2021-01-01\" #@param{type: \"string\"}\nend_date = \"2021-03-31\" #@param{type: \"string\"}\n\n\n#@title define queries\nbase_sql = f\"\"\"\nWITH\n  base AS (\n  SELECT\n    pat.*,\n    top_category,\n    sub_category,\n    naics_code\n  FROM\n    `{SG_table}` pat\n  LEFT JOIN\n    `{PLACES_table}`\n  USING\n    (placekey)\n  WHERE date(date_range_start) between date(\"{start_date}\") and date(\"{end_date}\") ),\n\n  join_sg_uc AS (\n  SELECT\n    sg.placekey,\n    sg.location_name,\n    case when sg.brands = \"\" or sg.brands is null then \"NN\" else sg.brands END brands,\n    sg.region,\n    sg.wkt_area_sq_meters,\n    sg.parent_placekey,\n    sg.date_range_start,\n    sg.date_range_end,\n    sg.top_category,\n    sg.sub_category,\n    naics_code || \"_\" || sub_category naics_code,\n    sg.raw_visit_counts sg_raw_visit_counts,\n    uc.raw_visit_counts uc_raw_visit_counts,\n    sg.raw_visitor_counts sg_raw_visitor_counts,\n    uc.raw_visitor_counts uc_raw_visitor_counts,\n    sg.visitor_home_cbgs sg_visitor_home_cbgs,\n    uc.visitor_home_cbgs uc_visitor_home_cbgs,\n    sg.distance_from_home sg_distance_from_home,\n    uc.distance_from_home uc_distance_from_home,\n    sg.median_dwell sg_median_dwell,\n    uc.median_dwell uc_median_dwell,\n    sg.normalized_visits_by_state_scaling sg_normalized_visits_by_state_scaling,\n    uc.normalized_visits_by_state_scaling uc_normalized_visits_by_state_scaling,\n    sg.normalized_visits_by_total_visits sg_normalized_visits_by_total_visits,\n    uc.normalized_visits_by_total_visits uc_normalized_visits_by_total_visits,\n    sg.normalized_visits_by_total_visitors sg_normalized_visits_by_total_visitors,\n    uc.normalized_visits_by_total_visitors uc_normalized_visits_by_total_visitors,\n    sg.normalized_visits_by_region_naics_visits sg_normalized_visits_by_region_naics_visits,\n    uc.normalized_visits_by_region_naics_visits uc_normalized_visits_by_region_naics_visits,\n    --sg.normalized_visitors_by_region_naics_visits sg_normalized_visitors_by_region_naics_visits,\n    --uc.normalized_visitors_by_region_naics_visits uc_normalized_visitors_by_region_naics_visits\n  FROM\n    base sg\n  JOIN\n    `{UC_table}` uc\n  ON\n    sg.placekey = uc.placekey\n    AND DATE(sg.date_range_start) = DATE(uc.date_range_start)\n  WHERE\n    naics_code IN {industries}\n    AND uc.date_range_start IS NOT NULL\n    AND sg.date_range_start IS NOT NULL ), \n\nnum_weeks_seen AS (\n  SELECT placekey, COUNT(DISTINCT date_range_start) as w_c FROM \n  join_sg_uc\n  GROUP BY placekey HAVING w_c > 7\n)\nSELECT \n  join_sg_uc.*\nFROM join_sg_uc\nINNER JOIN num_weeks_seen USING(placekey) \n\"\"\"\n\n\n#@title define functions\n\ndef add_trimmed_metrics(df):\n  \"\"\"\n    We go through each column and for metrics we trim values low and above 1 quantile.\n    This is applied on a per industry code basis and the resulting columns are \"_trimmed\"\n  \"\"\"\n\n  for i in df.columns:\n    \n    if df[i].dtypes == \"Int64\" or df[i].dtypes == \"float64\":\n      lb = df.groupby(\"naics_code\")[i].quantile(0.01)\n      ub = df.groupby(\"naics_code\")[i].quantile(0.99)\n      df = df.join(lb, on=\"naics_code\", how=\"left\", rsuffix=\"_lb\")\n      df = df.join(ub, on=\"naics_code\", how=\"left\", rsuffix=\"_ub\")\n      df[i + \"_trimmed\"]= df[i][(df[i] < df[i + \"_ub\"]) & (df[i] > df[i + \"_lb\"])]\n      df.drop([i + \"_lb\", i + \"_ub\"], axis=1, inplace=True)\n\n  return df\n\ndef plot_scatter(metric, df, time_inclusion = False, per_industry = \"ALL\"):\n  \"\"\"\n    Scatterplot using plotly express for selected metric with hardcoded hover_over.\n    If time_inclusion = True, then each weeks data will appear in the plot. So to\n    speak a regression including time.\n    If per_industry is defining \"brand\" or \"naics_code\" those will be used for color coding\n    and ensure separate processing per brand or naics_code. if \"ALL\" the is no split and everything goes in one.\n    Otherwise it will calculate the mean across time.\n  \"\"\"\n\n  if per_industry != \"ALL\":\n    if time_inclusion:\n      plot_data = df.copy().groupby([\"placekey\", \"date_range_start\", \"naics_code\",\"brands\", \"wkt_area_sq_meters\" ], as_index =False).mean()\n    else:\n      plot_data = df.copy().groupby([\"placekey\", \"naics_code\", \"brands\", \"wkt_area_sq_meters\"], as_index =False).mean()\n\n    fig = px.scatter(\n        plot_data, x='uc_' + metric, y='sg_' + metric, color=per_industry, opacity=0.8,\n        trendline='ols', hover_data=[\"brands\", \"wkt_area_sq_meters\"], template=\"unacast\"\n    )\n  else:\n    if time_inclusion:\n      plot_data = df.copy().groupby([\"placekey\", \"date_range_start\",\"brands\", \"wkt_area_sq_meters\" ], as_index =False).mean()\n    else:\n      plot_data = df.copy().groupby([\"placekey\", \"brands\", \"wkt_area_sq_meters\"], as_index =False).mean()\n\n    fig = px.scatter(\n        plot_data, x='uc_' + metric, y='sg_' + metric, opacity=0.8,\n        trendline='ols', hover_data=[\"brands\", \"wkt_area_sq_meters\"], template=\"unacast\"\n    )\n\n  fig.show()\n\n\ndef get_all_metrics(df):\n  \"\"\"\n    Get all unique metrics (int or float) from dataset\n  \"\"\"\n  list_columns = []\n  for i in df.columns:\n    if df[i].dtypes == \"Int64\" or df[i].dtypes == \"float64\":\n      if \"sg_\" in i or \"uc_\" in i:\n        add_column = i[3:]\n      else:\n        add_column = i\n      \n      if add_column not in list_columns:\n        list_columns.append(add_column) \n  \n  list_columns.sort()\n  return list_columns\n\n\ndef widget_scatter_plot(dataset):\n    \n    \"\"\"\n      Create widgets for metrics and analysis type to quickly change plots. \n      I stole Malachy's code and adjusted it :-) \n    \"\"\"\n    output = widgets.Output()\n    dropdown_metric = widgets.Dropdown(options = get_all_metrics(dataset), \n                                        value = None, description='metrics:')\n    dropdown_type = widgets.Dropdown(options = [\"place\", \"time\"], \n                                        value = None, description='analysis type:')\n    dropdown_industry = widgets.Dropdown(options = [\"ALL\", \"naics_code\", \"brands\"], \n                                        value = None, description='per industry:')\n    \n\n    def output_scatter(metric, analysis_type, industry):\n        try:\n            time_inclusion = False\n            if analysis_type == \"time\":\n              time_inclusion = True\n\n            output_data = plot_scatter(metric, dataset, time_inclusion, industry)\n \n            with output:\n                display(output_data)\n\n        except Exception as e:\n            IPython.display.clear_output(wait=True)\n            print(e)\n            display(input_widgets)\n        \n    def dropdown_metric_eventhandler(change):\n\n        display(input_widgets)\n\n        dropdown_metric = change.new\n        output_scatter(metric = dropdown_metric, \n                       analysis_type = dropdown_type.value, \n                       industry = dropdown_industry.value)\n        IPython.display.clear_output(wait=True)     \n\n    def dropdown_type_eventhandler(change):\n\n        display(input_widgets)\n\n        dropdown_type = change.new\n        output_scatter(metric = dropdown_metric.value, \n                       analysis_type = dropdown_type, \n                       industry = dropdown_industry.value)\n        IPython.display.clear_output(wait=True)   \n\n    def dropdown_industry_eventhandler(change):\n\n        display(input_widgets)\n\n        dropdown_industry = change.new\n        output_scatter(metric = dropdown_metric.value, \n                       analysis_type = dropdown_type.value, \n                       industry = dropdown_industry)\n        IPython.display.clear_output(wait=True) \n\n          \n    dropdown_metric.observe(dropdown_metric_eventhandler, names='value')\n    dropdown_type.observe(dropdown_type_eventhandler, names='value')\n    dropdown_industry.observe(dropdown_industry_eventhandler, names='value')\n\n    input_widgets = widgets.HBox([dropdown_industry, dropdown_type, dropdown_metric])\n  \n    display(input_widgets)\n    IPython.display.clear_output(wait=True)    \n\n\ndef get_metric_summary(df, split_column=\"naics_code\", top=None):\n  \"\"\"\n   1. Get covariance matrix and fetch correlation coefficients per metric.\n   2. create matrix with all correlations per placekey and split column\n   3. Bring in the percentage of places having > .55 correlation.\n   4. calculate percentage drop in volume from SG to UC\n  \"\"\"\n  correlation_summary = pd.DataFrame()\n\n  if top:\n    top_split_column = list(df.loc[df[split_column] != \"NN\"].groupby(split_column).count().nlargest(top, \"placekey\").index)\n    df = df.loc[df[split_column].isin(top_split_column)]\n\n  if split_column:\n    cov_matrix = df.groupby(split_column).corr()\n  else:\n    cov_matrix = df.corr()\n\n  list_columns = get_all_metrics(df)\n\n  list_columns.remove(\"wkt_area_sq_meters\")\n  list_columns.remove(\"wkt_area_sq_meters_trimmed\")\n\n  list_raw = [i for i in list_columns if \"_trimmed\" not in i]\n  cov_matrix.reset_index(level=0, inplace=True)\n\n  print(\"this is going to take a while since it runs correlations for all placekeys and naics codes...\")\n\n  temp_corr = df.groupby([\"placekey\", split_column]).corr().reset_index(level=1).reset_index(level=1)\n\n  for metric in list_raw:\n    temp_drop = (df.groupby(split_column)[\"sg_\" + metric].mean() - df.groupby(split_column)[\"uc_\" + metric].mean()) / df.groupby(split_column)[\"uc_\" + metric].mean() * 100\n    temp_drop_trimmed = (df.groupby(split_column)[\"sg_\" + metric + \"_trimmed\"].mean() - df.groupby(split_column)[\"uc_\" + metric + \"_trimmed\"].mean()) / df.groupby(split_column)[\"uc_\" + metric + \"_trimmed\"].mean() * 100\n    \n    for n_code in cov_matrix[split_column].unique():\n      correlation_summary = correlation_summary.append({'metrics': metric, \n                    split_column: n_code, \n\n                    # average correlation for raw and trimmed\n                    'avg_correlation_raw': cov_matrix[cov_matrix[split_column] == n_code][\"sg_\" + metric][\"uc_\" + metric], \n                    'avg_correlation_trimmed': cov_matrix[cov_matrix[split_column] == n_code][\"sg_\" + metric + \"_trimmed\"][\"uc_\" + metric + \"_trimmed\"], \n                    \n                    # get percentage of venues that are correlating > .55. the temp_corr comes with all possible combinations and needs some long filtering.\n                    'percent_>.55_raw': round(np.sum(temp_corr[(temp_corr[split_column] == n_code) & (temp_corr['level_1'] == \"sg_\" + metric)][\"uc_\" + metric] > .55) / np.sum(temp_corr[(temp_corr[split_column] == n_code) & (temp_corr['level_1'] == \"sg_\" + metric)][\"uc_\" + metric] >= -1), 2) * 100, \n                    'percent_>.55_trimmed': round(np.sum(temp_corr[(temp_corr[split_column] == n_code) & (temp_corr['level_1'] == \"sg_\" + metric + \"_trimmed\")][\"uc_\" + metric + \"_trimmed\"] > .55) / np.sum(temp_corr[(temp_corr[split_column] == n_code) & (temp_corr['level_1'] == \"sg_\" + metric + \"_trimmed\")][\"uc_\" + metric + \"_trimmed\"] >= -1), 2) * 100, \n                    \n                    # get mean percentage drop from SG to UC\n                    'percent_volume_drop_raw': round(np.nanmean(temp_drop[n_code])), \n                    'percent_volume_drop_trimmed': round(np.nanmean(temp_drop_trimmed[n_code]))\n                    }, ignore_index=True)\n      \n    print(f\"{metric} is done!\")\n  return correlation_summary\n\n\n#@title run queries\n\nquery_job = client.query(base_sql, job_config=None)\ndf = query_job.to_dataframe()\n\n#cast as float for some columns and add a trimmed version per industry\ndf['uc_distance_from_home'] = df['uc_distance_from_home'].astype(\"float64\")\ndf['sg_distance_from_home'] = df['sg_distance_from_home'].astype(\"float64\")\ndf = add_trimmed_metrics(df)\n\n\n#@title Widget\nwidget_scatter_plot(df)\n\n\n#@title get overall correlations of all metrics\ngrand_summary = get_metric_summary(df, \"naics_code\")\ngrand_summary.groupby([\"metrics\"]).mean()\n\n\nsummary_by_naics_code = get_metric_summary(df, \"naics_code\")\nsummary_by_brand = get_metric_summary(df, \"brands\", top=50)\nsummary_by_region = get_metric_summary(df, \"region\")\n\n\nimport plotly.express as px\nimport re\nfrom cartoframes.viz import Map, Layer, color_continuous_style\n\n\ndef metrics_per_group(summary, column, group, ndec=2):\n    summary[column] = summary[column].apply(lambda x: np.round(x, ndec))\n    op = summary[[\"metrics\", group, column]].pivot(index=\"metrics\", columns=group, values=column)\n    op.columns = [re.sub(\"[0-9]+_\", \"\", x) for x in op.columns]\n    op = op.reindex(sorted(op.columns), axis=1)\n    return op\n\n\ndef plot_metrics_per_group(metrics_per_group, title):\n    fig = px.imshow(metrics_per_group, \n                   text_auto=True, \n                   title = title,\n                   template=\"unacast\")\n    fig.update_layout(\n        yaxis={'title': \"\"},\n        margin={\"l\": 80, \"r\": 80, \"t\": 100, \"b\": 80},\n    )\n    fig.show()\n\n\nplot_metrics_per_group(metrics_per_group(summary_by_naics_code, \"avg_correlation_trimmed\", \"naics_code\"), title=\"Average correlation trimmed by Naics Code for metrics\")\nplot_metrics_per_group(metrics_per_group(summary_by_naics_code, \"percent_>.55_trimmed\", \"naics_code\"), title= \"Percent of placekeys with above .55 correlation by Naics Code for metrics\")\nplot_metrics_per_group(metrics_per_group(summary_by_naics_code, \"percent_volume_drop_trimmed\", \"naics_code\"), title=\"Percent volume drop trimmed by naics code for metrics\")\n\n\nplot_metrics_per_group(metrics_per_group(summary_by_brand, \"avg_correlation_trimmed\", \"brands\"), title=\"Average correlation trimmed by brand for metrics\")\nplot_metrics_per_group(metrics_per_group(summary_by_brand, \"percent_>.55_trimmed\", \"brands\"), title= \"Percent of placekeys with above .55 correlation by brand for metrics\")\nplot_metrics_per_group(metrics_per_group(summary_by_brand, \"percent_volume_drop_trimmed\", \"brands\"), title=\"Percent volume drop trimmed by brands for metrics\")\n\n\nplot_metrics_per_group(metrics_per_group(summary_by_region, \"avg_correlation_trimmed\", \"region\"), title=\"Average correlation trimmed by region for metrics\")\nplot_metrics_per_group(metrics_per_group(summary_by_region, \"percent_>.55_trimmed\", \"region\"), title= \"Percent of placekeys with above .55 correlation by region for metrics\")\nplot_metrics_per_group(metrics_per_group(summary_by_region, \"percent_volume_drop_trimmed\", \"region\"), title=\"Percent volume drop trimmed by region for metrics\")\n\n\nget_region_polygons = client.query(\"\"\"\nSELECT iso.Abbreviation as region, geog FROM `uc-atlas.maps_us.states_iso` iso\nLEFT JOIN `uc-atlas.maps_us.states` pol ON iso.State = pol.state_name\n\"\"\").to_dataframe()\n\n\ndef metric_by_region_map(summary, region_poly, column):\n    maps = {}\n    geog_df = summary.merge(region_poly, on=\"region\", how=\"inner\")\n    for k, grp in geog_df.groupby(\"metrics\"):\n        maps[k] = Map([\n                    Layer(\n                        grp, \n                        geom_col=\"geog\", \n                        style=color_continuous_style(value=column),\n                        title = f\"{column} for metric {k}\"\n                    )\n                    ]\n                )\n    return maps\n\n\nmetric_by_region = metric_by_region_map(summary_by_region, get_region_polygons, \"avg_correlation_trimmed\")\n\n\nmetric_by_region[\"raw_visit_counts\"]\n\n\nfor n_code in grand_summary[\"naics_code\"].unique():\n  df_plot = grand_summary[grand_summary[\"naics_code\"] == n_code].drop(\"naics_code\", axis=1).set_index('metrics').transpose().stack().reset_index(level=0).copy()\n  df_plot.reset_index(level=0, inplace=True)\n  df_plot.columns = [\"metrics\", \"features\", \"values\"]\n\n  fig = px.line_polar(df_plot, r=\"values\", theta=\"features\", color=\"metrics\", line_close=True,\n                    template=\"unacast\")\n  fig.show()\n\n\n#grand_summary.reset_index(level=0, inplace=True)\n#grand_summary.columns = [\"metrics\", \"avg_corr_raw\", \"avg_corr_trim\", \"%_>.55_raw\", \"%_>.55_trim\", \"%_vol_drop_raw\", \"%_vol_drop_trim\"]\ndf_test = grand_summary.set_index('metrics').transpose().stack().reset_index(level=0).copy()\ndf_test.reset_index(level=0, inplace=True)\ndf_test.columns = [\"metrics\", \"features\", \"values\"]"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  }
]